{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Review Sentiment Baseline Performance\n",
    "This notebook establishes baseline performance on binary sentiment classification using a simple model.  The data for this example comes from [Andrew Maas](http://ai.stanford.edu/~amaas/data/sentiment/).  We use sklearn to tokenize the reviews, create a TF-IDF feature vector, and a logistic regression model with Ridge regularization. There is also a custom implementation of the NB-SVM model from the Baselines and Bigrams paper which gets slightly better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, urllib, tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tar file already extracted.\n"
     ]
    }
   ],
   "source": [
    "DATA_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATA_DIR = './data'\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "if not os.path.isfile(os.path.join(DATA_DIR,'movie_data.tar.gz')):\n",
    "    urllib.request.urlretrieve(DATA_URL, os.path.join(DATA_DIR,'movie_data.tar.gz'))\n",
    "else:\n",
    "    print(\"Data already downloaded.\")\n",
    "\n",
    "if os.path.isfile(os.path.join(DATA_DIR,'movie_data.tar.gz')) and not os.path.exists(os.path.join(DATA_DIR,'aclImdb')):\n",
    "    f = tarfile.open(os.path.join(DATA_DIR,'movie_data.tar.gz'))\n",
    "    f.extractall(path=DATA_DIR)\n",
    "    f.close()\n",
    "else:\n",
    "    print(\"Tar file already extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train/Test IMDB Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_FOLDER = 'data/aclImdb/train/'\n",
    "TEST_DATA_FOLDER = 'data/aclImdb/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "def create_dataframe_from_files(data_folder):\n",
    "    examples = list()\n",
    "    for d in ['pos','neg']:\n",
    "        for f in os.listdir(os.path.join(data_folder,d)):\n",
    "            _tmp = open(os.path.join(data_folder,d,f),'r', encoding='utf-8')\n",
    "            if d=='pos':\n",
    "                examples += [(_tmp.read(),f,1)]\n",
    "            else:\n",
    "                examples += [(_tmp.read(),f,0)]\n",
    "    df_tmp = pd.DataFrame(examples, columns=['text','file','target'])\n",
    "    df_tmp = df_tmp.sample(frac=1)\n",
    "    df_tmp = df_tmp.reset_index(drop=True)\n",
    "    return df_tmp\n",
    "                \n",
    "df_train = create_dataframe_from_files(TRAIN_DATA_FOLDER)\n",
    "df_test = create_dataframe_from_files(TEST_DATA_FOLDER)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Test and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(encoding='utf-8',\n",
    "                                   decode_error='strict',\n",
    "                                   strip_accents=None,\n",
    "                                   lowercase=False,\n",
    "                                   preprocessor=None,\n",
    "                                   tokenizer=None,\n",
    "                                   stop_words='english',\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   max_features=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100000)\n",
      "(25000, 100000)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = tfidf_vectorizer.fit_transform(df_train['text'])\n",
    "X_test = tfidf_vectorizer.transform(df_test['text'])\n",
    "y_train = df_train['target'].values\n",
    "y_test = df_test['target'].values\n",
    "y_test_rand = df_test['target'].sample(frac=1).values\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Ridge Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_baseline_performance(y_test, y_test_rand, y_hat, y_hat_proba=None):\n",
    "    print('Random baseline: {0:.3f}'.format(accuracy_score(y_test, y_test_rand)))\n",
    "    print('Ridge model accuracy: {0:.3f}'.format(accuracy_score(y_test, y_hat)))\n",
    "    if y_hat_proba is not None:\n",
    "        print('ROC AUC: {0:.3f}'.format(roc_auc_score(y_test, y_hat_proba)))\n",
    "    print('Confusion Matrix:')\n",
    "    print(confusion_matrix(y_test, y_hat))\n",
    "    print('Report: \\n')\n",
    "    print(classification_report(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_model = RidgeClassifierCV(alphas=(0.1, 0.5, 2.0, 5.0, 10.0),\n",
    "                                fit_intercept=True,\n",
    "                                normalize=False, \n",
    "                                cv=5,\n",
    "                                class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifierCV(alphas=(0.1, 0.5, 2.0, 5.0, 10.0), class_weight=None, cv=5,\n",
       "         fit_intercept=True, normalize=False, scoring=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = ridge_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline: 0.505\n",
      "Ridge model accuracy: 0.873\n",
      "Confusion Matrix:\n",
      "[[11031  1469]\n",
      " [ 1702 10798]]\n",
      "Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      0.88      0.87     12500\n",
      "          1       0.88      0.86      0.87     12500\n",
      "\n",
      "avg / total       0.87      0.87      0.87     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_baseline_performance(y_test, y_test_rand, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB-SVM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is inspired by the [Baselines and Bigrams](https://nlp.stanford.edu/pubs/sidaw12_simple_sentiment.pdf) paper, and this [Kaggle notebook](https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline).  This model will generally do a bit better than just Ridge with TF-IDF, and would be good to use as a baseline on any real projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "        self.coef_ = None\n",
    "\n",
    "    def predict(self, x):\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        #y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            '''Sum the X vectors by column for a target class, then return result normalized by count'''\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        self.coef_ = self._clf.coef_\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbsvm_model = NbSvmClassifier(C=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NbSvmClassifier(C=8.0, dual=False, n_jobs=1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbsvm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = nbsvm_model.predict_proba(X_test)\n",
    "y_hat_proba = y_hat[:,1]\n",
    "y_hat = y_hat[:,1] > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random baseline: 0.503\n",
      "Ridge model accuracy: 0.888\n",
      "ROC AUC: 0.956\n",
      "Confusion Matrix:\n",
      "[[11091  1409]\n",
      " [ 1392 11108]]\n",
      "Report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.89      0.89      0.89     12500\n",
      "          1       0.89      0.89      0.89     12500\n",
      "\n",
      "avg / total       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_baseline_performance(y_test, y_test_rand, y_hat, y_hat_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
